{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1IJzYYGXGk1sa14unmWX_2XeeeLhGoA0R",
      "authorship_tag": "ABX9TyP7WEQbhyGTEXl+jDCj3NyA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dariiazorii/OID/blob/main/lab1_oid/LAB1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qppwrKpfX-kU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkRZ3uckXCDL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc29786-a9cc-42e3-d19f-f6982214fd0c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/4: Встановлення Spark 3.3.4...\n",
            "^C\n",
            "^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m2/4: Налаштування змінних середовища...\n"
          ]
        }
      ],
      "source": [
        "# 1. Перезапуск та встановлення Spark 3.3.4\n",
        "print(\"1/4: Встановлення Spark 3.3.4...\")\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.3.4/spark-3.3.4-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.4-bin-hadoop3.tgz\n",
        "!pip install -q findspark pyspark\n",
        "\n",
        "# 2. Налаштування середовища (Критично: зміна шляху SPARK_HOME)\n",
        "print(\"2/4: Налаштування змінних середовища...\")\n",
        "import os\n",
        "import findspark\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.4-bin-hadoop3\" # Зміна шляху!\n",
        "findspark.init()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Монтування Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "id": "RfXuCRzwMd3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Створення SparkSession та робота\n",
        "print(\"4/4: Створення SparkSession та перетворення...\")\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"LAB_1\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "csv_file = \"/content/drive/MyDrive/Colab Notebooks/oid/2019-Oct.csv\"\n",
        "parquet_output = \"/content/2019_Oct_parquet\"\n",
        "\n",
        "df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
        "df.write.parquet(parquet_output, mode=\"overwrite\")\n",
        "print(\"Перетворення CSV у Parquet успішно завершено.\")\n",
        "print(\"PySpark встановлено та налаштовано. Можна створювати SparkSession.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhxywml-Lupd",
        "outputId": "1f5a3c7d-df15-4998-c6b4-8cd2f287540c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4: Створення SparkSession та перетворення...\n",
            "Перетворення CSV у Parquet успішно завершено.\n",
            "PySpark встановлено та налаштовано. Можна створювати SparkSession.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gzip_parquet_output = \"/content/2019_Oct_parquet_gzip\"\n",
        "\n",
        "print(f\"Створення Parquet з Gzip стисненням до: {gzip_parquet_output}...\")\n",
        "\n",
        "\n",
        "df.write.format(\"parquet\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"compression\", \"gzip\") \\\n",
        "    .save(gzip_parquet_output)\n",
        "\n",
        "print(\" Gzip-стиснення завершено.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKF4wl98demX",
        "outputId": "65dcf265-c443-4cf4-c9f3-193c49f9744d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Створення Parquet з Gzip стисненням до: /content/2019_Oct_parquet_gzip...\n",
            " Gzip-стиснення завершено.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh \"/content/drive/MyDrive/Colab Notebooks/oid/2019-Oct.csv\"\n",
        "!du -sh \"/content/2019_Oct_parquet\"\n",
        "!du -sh \"{gzip_parquet_output}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_3Pwwq0Um9b",
        "outputId": "8f2d4eb2-aeb2-48ee-b3cf-3cab165d1eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.3G\t/content/drive/MyDrive/Colab Notebooks/oid/2019-Oct.csv\n",
            "1.4G\t/content/2019_Oct_parquet\n",
            "926M\t/content/2019_Oct_parquet_gzip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "\n",
        "csv_path = \"/content/drive/MyDrive/Colab Notebooks/oid/2019-Oct.csv\"\n",
        "snappy_path = \"/content/2019_Oct_parquet\"\n",
        "gzip_path = \"/content/2019_Oct_parquet_gzip\"\n",
        "\n",
        "def run_and_time_count(path, format_name):\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    if \"csv\" in path:\n",
        "\n",
        "        df = spark.read.csv(path, header=True, inferSchema=True)\n",
        "    else:\n",
        "\n",
        "        df = spark.read.parquet(path)\n",
        "\n",
        "\n",
        "    count = df.count()\n",
        "\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    print(f\"  {format_name} (Кількість рядків: {count}): {duration:.2f} сек.\")\n",
        "    return duration\n",
        "\n",
        "print(\"--- Порівняння продуктивності: Підрахунок рядків ---\")\n",
        "\n",
        "# 1. CSV (Оригінал)\n",
        "time_csv = run_and_time_count(csv_path, \"1. CSV (Оригінал)\")\n",
        "\n",
        "# 2. Parquet (Snappy)\n",
        "time_snappy = run_and_time_count(snappy_path, \"2. Parquet (Snappy)\")\n",
        "\n",
        "# 3. Parquet (Gzip)\n",
        "time_gzip = run_and_time_count(gzip_path, \"3. Parquet (Gzip)\")\n",
        "\n",
        "print(\"\\n--- Висновок ---\")\n",
        "print(f\"Parquet  у {time_csv / time_snappy:.1f} рази швидше, ніж CSV.\")\n",
        "print(f\"Parquet (Gzip) у {time_csv / time_gzip:.1f} рази швидше, ніж CSV.\")\n",
        "print(f\"Parquet (Gzip) у {time_snappy / time_gzip:.1f} рази швидше, ніж Parquet (Snappy).\")\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Idq4mre0fesl",
        "outputId": "c46891fe-4b5d-4978-b8a6-6eb75460c4ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Порівняння продуктивності: Підрахунок рядків ---\n",
            "  1. CSV (Оригінал) (Кількість рядків: 42448764): 297.81 сек.\n",
            "  2. Parquet (Snappy) (Кількість рядків: 42448764): 1.17 сек.\n",
            "  3. Parquet (Gzip) (Кількість рядків: 42448764): 0.68 сек.\n",
            "\n",
            "--- Висновок ---\n",
            "Parquet (Snappy) у 254.1 рази швидше, ніж CSV.\n",
            "Parquet (Gzip) у 437.3 рази швидше, ніж CSV.\n",
            "Parquet (Gzip) у 1.7 рази швидше, ніж Parquet (Snappy).\n"
          ]
        }
      ]
    }
  ]
}